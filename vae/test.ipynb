{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders for collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
    "\n",
    "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create train/validation/test splits following strong generalization: \n",
    "\n",
    "- We split all users into training/validation/test sets. \n",
    "\n",
    "- We train models using the entire click history of the training users. \n",
    "\n",
    "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset at http://files.grouplens.org/datasets/movielens/ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
    "DATA_DIR = '/Users/andyliu/ml-20m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112485573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n",
    "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" %\n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize_old(tp):\n",
    "    uid = map(lambda x: profile2id[x], tp['userId'])\n",
    "    sid = map(lambda x: show2id[x], tp['movieId'])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "def numerize(tp):\n",
    "    #https://github.com/dawenl/vae_cf/issues/10\n",
    "    uid = [profile2id[x] for x in tp['userId']]\n",
    "    sid = [show2id[x] for x in tp['movieId']]\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDAE(object):\n",
    "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "\n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.construct_placeholders()\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        self.input_ph = tf.compat.v1.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
    "        self.keep_prob_ph = tf.compat.v1.placeholder_with_default(1.0, shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.construct_weights()\n",
    "\n",
    "        saver, logits = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        # per-user average negative log-likelihood\n",
    "        neg_ll = -tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "            input_tensor=log_softmax_var * self.input_ph, axis=1))\n",
    "        # apply regularization to weights\n",
    "        #OLD REGULARIZATION (NOT USEFUL)\n",
    "        '''reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)'''\n",
    "        \n",
    "        reg = tf.keras.regularizers.l2(self.lam)\n",
    "        penalties = [reg(w) for w in self.weights]\n",
    "        penalties = [p if p is not None else constant_op.constant(0.0) for p in penalties]\n",
    "        for p in penalties:\n",
    "            if p.get_shape().ndims != 0:\n",
    "                raise ValueError('regularizer must return a scalar Tensor instead of a '\n",
    "                         'Tensor with rank %d.' % p.get_shape().ndims)\n",
    "        summed_penalty = tf.math.add_n(penalties)\n",
    "        reg_var = summed_penalty\n",
    "        \n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        loss = neg_ll + 2 * reg_var\n",
    "\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.compat.v1.summary.scalar('loss', loss)\n",
    "        merged = tf.compat.v1.summary.merge_all()\n",
    "        return saver, logits, loss, train_op, merged\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # construct forward graph\n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, 1 - (self.keep_prob_ph))\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return tf.compat.v1.train.Saver(), h\n",
    "\n",
    "    def construct_weights(self):\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # define weights\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
    "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_{}\".format(i+1)\n",
    "\n",
    "            self.weights.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(MultiDAE):\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        super(MultiVAE, self).construct_placeholders()\n",
    "\n",
    "        # placeholders with default values when scoring\n",
    "        self.is_training_ph = tf.compat.v1.placeholder_with_default(0., shape=None)\n",
    "        self.anneal_ph = tf.compat.v1.placeholder_with_default(1., shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._construct_weights()\n",
    "\n",
    "        saver, logits, KL = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        neg_ll = -tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "            input_tensor=log_softmax_var * self.input_ph,\n",
    "            axis=-1))\n",
    "        # apply regularization to weights\n",
    "        '''reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)'''\n",
    "        \n",
    "        reg = tf.keras.regularizers.l2(self.lam)\n",
    "        penalties = [reg(w) for w in self.weights_q + self.weights_p]\n",
    "        penalties = [p if p is not None else constant_op.constant(0.0) for p in penalties]\n",
    "        for p in penalties:\n",
    "            if p.get_shape().ndims != 0:\n",
    "                raise ValueError('regularizer must return a scalar Tensor instead of a '\n",
    "                         'Tensor with rank %d.' % p.get_shape().ndims)\n",
    "        summed_penalty = tf.math.add_n(penalties)\n",
    "        reg_var = summed_penalty\n",
    "        \n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
    "\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.compat.v1.summary.scalar('KL', KL)\n",
    "        tf.compat.v1.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
    "        merged = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "        return saver, logits, neg_ELBO, train_op, merged\n",
    "\n",
    "    def q_graph(self):\n",
    "        mu_q, std_q, KL = None, None, None\n",
    "\n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, 1 - (self.keep_prob_ph))\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights_q) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                mu_q = h[:, :self.q_dims[-1]]\n",
    "                logvar_q = h[:, self.q_dims[-1]:]\n",
    "\n",
    "                std_q = tf.exp(0.5 * logvar_q)\n",
    "                KL = tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "                        input_tensor=0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
    "        return mu_q, std_q, KL\n",
    "\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # q-network\n",
    "        mu_q, std_q, KL = self.q_graph()\n",
    "        epsilon = tf.random.normal(tf.shape(input=std_q))\n",
    "\n",
    "        sampled_z = mu_q + self.is_training_ph *\\\n",
    "            epsilon * std_q\n",
    "\n",
    "        # p-network\n",
    "        logits = self.p_graph(sampled_z)\n",
    "\n",
    "        return tf.compat.v1.train.Saver(), logits, KL\n",
    "\n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            if i == len(self.q_dims[:-1]) - 1:\n",
    "                # we need two sets of parameters for mean and variance,\n",
    "                # respectively\n",
    "                d_out *= 2\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "\n",
    "            self.weights_q.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases_q.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases_q[-1])\n",
    "\n",
    "        self.weights_p, self.biases_p = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases_p.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights_p[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases_p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = range(N)\n",
    "\n",
    "# training batch size\n",
    "batch_size = 500\n",
    "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
    "\n",
    "N_vad = vad_data_tr.shape[0]\n",
    "idxlist_vad = range(N_vad)\n",
    "\n",
    "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
    "batch_size_vad = 2000\n",
    "\n",
    "# the total number of gradient updates for annealing\n",
    "total_anneal_steps = 200000\n",
    "# largest annealing parameter\n",
    "anneal_cap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-VAE^{PR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dims = [200, 600, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.compat.v1.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.compat.v1.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory\n",
    "\n",
    "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
    "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /Users/andyliu/Downloads/vae_cf-master/200.0\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(log_dir, graph=tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /Users/andyliu/Downloads/vae_cf-master/200.0\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    update_count = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(list(idxlist))\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "\n",
    "            feed_dict = {vae.input_ph: X,\n",
    "                         vae.keep_prob_ph: 0.5,\n",
    "                         vae.anneal_ph: anneal,\n",
    "                         vae.is_training_ph: 1}\n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train,\n",
    "                                           global_step=epoch * batches_per_epoch + bnum)\n",
    "\n",
    "            update_count += 1\n",
    "\n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "\n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "\n",
    "batch_size_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0)\n",
    "saver, logits_var, _, _, _ = vae.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /volmount/chkpt/ml-20m/VAE_anneal200K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.42592 (0.00211)\n",
      "Test Recall@20=0.39535 (0.00270)\n",
      "Test Recall@50=0.53540 (0.00284)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> n_items] MLP, thus the overall architecture for the Multi-DAE is [n_items -> 200 -> n_items]. We find this architecture achieves better validation NDCG@100 than the [n_items -> 600 -> 200 -> 600 -> n_items] architecture as used in Multi-VAE^{PR}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_dims = [200, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.compat.v1.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.compat.v1.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in dae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /volmount/log/ml-20m/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(log_dir, graph=tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /volmount/chkpt/ml-20m/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            feed_dict = {dae.input_ph: X,\n",
    "                         dae.keep_prob_ph: 0.5}\n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum)\n",
    "\n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "\n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAADQCAYAAAA56sZ8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW5x/HPmS0zk5nsEzYBkSJocAPlqriBYrXLFa1Y\n3HJtqWIVxKoFzBVDW0XAq95ibZVarSJtY1P0equttra9V9sYN0Sl9aJYkTWZJJN9JrOd+8eEUSQM\nGWSYLN/368XLzJk55zzz5Dh55nee8zuGaZomIiIiIiJyUFmyHYCIiIiIyECkQltEREREJANUaIuI\niIiIZIAKbRERERGRDFChLSIiIiKSASq0RUREREQywJbtADLB72/L2r4LC90EAp1Z239/o3ylTzlL\nj/KVPuUsPcpX+pSz9Chf6TuUOfP5vPt8TiPaB5nNZs12CP2K8pU+5Sw9ylf6lLP0KF/pU87So3yl\nr6/kTIW2iIiIiEgGqNAWEREREckAFdoiIiIiIhmgQltEREREJAMyOuvIsmXL2LBhA4ZhUFFRwbHH\nHrvXa+655x7eeust1qxZA8DKlSt54403iEajzJ07l3PPPZfFixezceNGCgoKAJgzZw5nnXVWJkMX\nEREZdEzTxDTBYjEO6jYDbV047FZynTYMw0guD3bFsFkNHPZDd+FaJBqnoSVIU1sXsZhJ3DTBBFeO\nFbfTjsthJRKLE47EicTieF12Cjw55Dj2H6NpmoSjcbrCMUKRGBYDHHYrOXYrDpsl+d570toZZldj\nJ83tXQTaurDbLJwwzkehN+eT13SEqW8O4sqx4XHacDvt2G2fjJkGu6Js87fT0BIiL9dBcZ6TIm9O\nyvzGTZNAaxehSIxYLE7cNLFbLbhybDgdVuw2C1aLJa1jItqdv7hpYrMa2KwWItE4ze1dNLV10RmK\nYnzqtc3tYZrbu+gIRrDbLDjsViwWg85QhPZglGBXFKslsR2bzYLHaSPXZcfttBGOxAl2RYnFTL74\nLyMpyXf1Os5DIWOF9quvvsqWLVuoqqpi8+bNVFRUUFVVtcdrPvjgA1577TXsdjsAr7zyCu+//z5V\nVVUEAgEuvPBCzj33XABuuukmpk2blqlwRUR6xTRNOkJRXDlWrJZP/sBFonE+2tVKNGbicdnxuOzk\n5dr3eM2+theNxQmGY3R1/wuFYwDk5drJ9+Rgt1loD0ZobQ/T0hGmpaOLlo4woa4YebkOCjwOnDk2\nGpqD7GrqpLG1C4fNgsthw5ljJRZP7MOMw/ASN2NH5DPCl4vVYknuv6ElRF1TkPrmIPl5TojF8bjs\nRKJxWjq6aO0I43baGT3Ey2GluVgtBvWBIHWBIF3hGA67Facjsa+2zjCtHYlYd/8cN2FESS6HlXrI\ny3Xgbw5S3xSksTVEezBCezBCOBrrzpsDj8tOPG4SicaJx01cThsepx2P257Mb67LjsthI6e7GGhq\nCbGzqZO6pk6CXVEisTiRaOJ9lOQ7Kcl3YRjQEYrSGYpgmuDo/qMOEInFiUbjyfUi0TgWw8DltJHr\ntGGa0NQaoqmti7bOcPdrE7ENKXQxeoiXknwnLR1hmlpDtLSHCUVihCPdv9tILFGEdT8OR2JEYmZ3\nIeegwJtDgSfxLy/XTqCtix0NndQFOonHTew2C3abhXAkTkcoQkcwgsft4DBfLiN8HmwWg0B7F83t\nYTqCEULhGKFwFKvFQn5uIq+5Tjs2mwWb1UIsHqe1I0xbZ4SWjnD3z2HA4MiR+ZQdXsTIIR6aWruo\nDwRpaAkmf1ehcIyc7t+5w26lKxwj2BWlKxKjJN/JsOJcSvKdbK1v572PAzS3hwFw2C0Uep3E4ibN\nbSGiMRMAu81CrtOGw27FajGwWgwiMZOucJRQOEY8bmIYBhYLWAwj8bMBebkOxo0sYPzIAvJzHWxv\n6GBbfTv+lhCdoQidoSjh7t+j1WoQi8VpauvCNNP/f9+Vs7tgThxvse7jM7r7eOn+775YLQYetx2v\ny4HXbe/+56AjGGHzjhb8zaG91ln7wibGjSxg7MgC3t7kZ3tDx16vcdgt5DrtWAyDxta9t2EAvkIX\nI30ehpW4icUTX3DagxHqmzrZ2dSZMu7kdgywWizYrEay6LVaDWwWC3HTJNx9fO8usLNh7GF5g6fQ\nrqmp4ZxzzgFg7NixtLS00N7ejsfjSb5m+fLlfOc73+FHP/oRACeddFJy1DsvL49gMEgsFstUiCLS\nS62dYeqaOhla5MbrdiSXR2Nx/M1BdjR0sKOxk/pAJ4ZhkGOz4nBYKPI6GVLowlfowmoYyT9ETocV\nr9uB02HFMAwi0TjBcBToLnxsVoLhKI0tIRpbQ3v8NxSJYbcmCo7d/7XZLMRiJo2tIZpaQwS7onjd\nDvJzHZQUuYnH4titFkwTGltD+JuDNLd3YRi7/2AYuHPs5LpsuHJshMKJP0LBrihOu5Vclx2nw0pz\nexe7moIEu6I4bBYOK/VwmM+DvznI5u0thD/zx8piGBTl5eArcJFjtxLqLhq6IrHuIihRcO3vj5Jh\ncECFQSoOW+KPZFc4/T+Ku0ei0g3pH1sCPS63GAa5LhsOm5W6piAf17WnueXssVktxONx/v7P3r7e\nSBRrdisupx2v1aC9M8L721r2mU+b1cBiMYhE4ph8ki93jo2m1hA7GjrgH/U9xuZ0WInG4mzzp86p\n3WYhz+1gZKmXSDTG3z8K8PePev59OeyJL3FtnYkveyaJY9SdY8Nms/Dex82893Fz8vV5bjuTj/QR\ni5s0tYYItHfhzLExstSD1+0gFjfpCEboCEXoiiSK6lgsMRLqdNjIczuwWg3i8cToa9w0icdN4ibU\nB4Js83fw5ze39/ie3Dk2HHYL8bhJVziOxWIwbkQ+pYVuivOd3UVj4stwZ1fiC1goHEuMqtqs2KwG\nbZ0RAu1dtLR3dX9ZitERimC1WrBbDZw5NrxuC3ZbovjMcVhxdo9ix00IRxP/z3eGorR1hmloCe71\n+8h12ph4RBEjSz0UeZ0UeBw0t4d57R91vL+1mU1bm3HYLJQdXsjIUi+hSCyZs45QlI5ghEgszlGj\nCxlZ6qG00EVbZyTxeRdI7O+NTX7Y9Jnfpc3C8OJchhW7ceXYsOz+ktP9mRzqihGJxYnF4kS7fy+x\nWDzx5T2e+DkcjWGxGLiddgq6v7jabRZy7FYMg+QXfavFQqHXQaHXSa4zUX6agM1iJL5genPIddkT\nX2C7v8C4nTY8LjuuHBumaRKJmkSiMTpCUdqDiS9SDrsl+bq+VmRDBgvthoYGysrKko+Liorw+/3J\nQnvdunVMmTKFESNGJF9jtVpxu90AVFdXc8YZZ2C1JkYannjiCR599FGKi4tZsmQJRUVF+9x3YaE7\nq/Mnppq4XPY20PJlmmbig7k1lCxgrBaDocW5yZGzWNzkg60B3tncSHNb4nRZZ1eEkgIXhw/NY+QQ\nL/5AkE1bA3y4vQWb1UJxvpPifFdiVMqVOGVomtAVjhIMJ4rPAm8Ohd4cWtrD/P2jRt77qInWjjD5\nuTnkeRyJkdHOxIhUMBQlHI0lR+0KvIkPurxcR3KkIhiK8o+PmthW/8kfhdIiN4eVevAHguxsaE+O\nSB0Ih82CCb0aTektV05i9HF7Qwcf7WqDzY17vcbpsFKc7wQSv4twJM6uQCdddZ98sd99mrutM8LH\n3e/fZrUwrCSXIUVumlpCbNnVyoc7WgE4fFgeE8cW43E5aO0eAW5sCbGrsWOPAtNht+LKseLKsZHv\nyUmennXmJIr83Y8BAm1dBFpDhMKx5O+2wOuksPtnl9NGS1uYprYQHcEIpYVuRvg8DCl2E4nG6QxF\nkqdc7TYrcdPkn9tbeG9LgA+2NYMJOQ4rOQ4rJfkuhvtyGVaSCyROUbd2hHHYEsdVvieHlvYuNm9v\n4Z/bWzExGeHzMLwkl1yXPfHFoSuKsftY6v7DWeDJId+bQzxu8vGuVj7a2UpzexfDinMZXuJhaLGb\nXJd9j1Pqoa4obZ0RrFYDu82CxTDoCEZo7QzT1j3q2tYRprUzQqgrcVo5FI5SUuDisFIPI3wevLmO\n7kLJQktHF3VNndQ3dWIAHrcDjzuxz90jzbuPx2SBZbPgsCdGLTuDiULGxKSkwEVJgYtCbw42a6IV\nIByJsWVXK5u3tdDQEqQ4z0lJgYuiPCcup6175Dfxe7Vaez7DEY3FaW7roqn7C2VzexdF3hxGDvUy\npChxBiFx9iFRgH66/aI+EGTLrsRxWJTnpDjPicft2KOloCsSS37WRKKx5ChvYff/964c2x6/g0Bb\niA2b/GzztzOk0M2wklyGFueSl+vYow1hd6vEp9siQuEo2+vbqWvqZOQQL4eVelK2THwekWiczdua\neWdzA+2dEUYPy2PM8DyGleTidPTde/KFI7HEWZ/2Llw5NoaV5PaYo9nnHUVjS5DGlhBjhufv8TtN\nh2kmvuRs97djt1pxuxKFaaHXeVDbhPqavlBfGKaZmfH9JUuWcOaZZyZHtS+99FKWLVvGmDFjaG5u\nZt68eTz66KPU1dVx6623Jnu0Af74xz/y0EMP8cgjj+D1eqmpqaGgoICjjjqK1atXs2vXLm6//fZ9\n7jubd4b0+bxZ3X9/83nyFTdNLGl8eIcjMUwT7PbEH27TTBRYnV1RdjZ2sHl7C5t3tNLSESbHZsHh\nsOLOsZGfm0OBx4HVakmOrCZGNhLf5KOxODZLYlQ18WHWlfzD/WkWw2BYcWIU5cMdrbQHIwf0vtOV\n40ic1v00A3A4Ej2DNmvitF9bR6THkc0ch5UvjMhnREkuOxo72LKrjbbOCK4ca6JYKs5lWImbYcW5\nDC1yYxgQjsQJdY9I1weC+JuDmJAchQ6Go8nT1RbDwJ2TKDQN6D71mDgtXZyfKBo+/V9Xjo3o7lP8\nnzpdaxgGxXmfFAy7+z8dLgd19a3J0ebiPCdet73HP2qRaIxgVyx5Ony3aCxxnHic9j3+KEWicXY1\ndZLvcZD3qZH+z9o9SpfT3XfY1+lzLD3KV/qUs/QoX+k7lDlLVdBn7OteaWkpDQ0Nycf19fX4fD4g\n0Yvd1NTE5ZdfTjgc5uOPP2bZsmVUVFTw0ksv8eCDD/Lwww/j9SYCP+WUU5LbmT59OkuXLs1U2JIB\nuwtaAAyIx02CXVE6u6LUt4XZUddKMJR4vPuUejQaJy/XQVFeDl6Xneb2MP6WRMHmb+4+9d/WRVGe\nkzHDvBw+LI8cu5VINFH8tgcjtCT7WcO0dnQR7Pqk2Eyc7jV7LCwdNsteLQCfZbXsPv2buEikq/tU\nIkBpoYuSfCcFnpxkURWJxhK9g/4Otjd0UOjN4YzjhjNxTBElBU7cOYlRL39LiG3+dnY2dFLozWHM\nMC+jh3oxSPReBlpDWB12dvnbEheTGN0jkt3vffcopDMnURyPHZFPrtNOJBqjrTNCNBZP9rV+tuCL\nm2ai/7Izkjh1GzexWgyGlbj36DNOFLDRvUbAejLusJRPH7CcXlw4ZRgGbqcNX0kuNrN3I+Z2mxV7\nD2fDbFZLj4W03WZhZKlnr+UHEq+IiAw8GSu0p06dyv3338/s2bPZuHEjpaWlybaR8847j/POOw+A\nbdu2ceutt1JRUUFbWxsrV67k5z//eXKGEYD58+ezcOFCRo4cSW1tLePGjctU2PI5xU0Tf3Oix3LL\nrja21LWxZVfbQR29NYCivByOGJFHXVOQ1//Pz+v/59/na71uO8V5LvJz7VitlsSFSZE4VovRfare\nSnGeky+MyOeIEfnk5zqImyaR7guOWjrCtLSHicbi3e0bTryunkdE9ydumrR3RvY5oprvyeELI/J7\nXHdokZuhRe4D+pZut1kpyktd7FkMgzx36pFZ2F3A2tPav4iIyGCUsUJ70qRJlJWVMXv2bAzDoLKy\nknXr1uH1epkxY0aP6zz33HMEAgFuvPHG5LIVK1Zw+eWXc+ONN+JyuXC73dx1112ZCntQi3f3+tUH\ngok+wdYQXZEYZvcFKLsv5ugMRQhH490XpCRGOBMXqLDXyDFASb6Tw4d6E6fzMbtbBWy4nDZKCt0Q\ni+NyJi7scToSvYxWi0FrR5imtkSva4HHga/Aha+773F3n5ppJi6A+7iunXjcxNbdmuBx2cn3JK7s\n3t+sDz2xGEayd7Uoz3lQ8rt7u3m5qQtZERERGRgy1qOdTerRTs3sHnXe2dhJXSBIXaCTrfXtbK1r\n77G3+LMMEqfMDYuBpXuKJYslcXFOrtPG6KFeRpUmWh5GDfGQm2L0sz/kq69RztKjfKVPOUuP8pU+\n5Sw9ylf6BnyPtvQtu5o6eWdzI5u2NvP+tmZaO/ds5bAYiV7cUaVehha7Kc7LocjrxN19c4Hdk+7n\nOm04c2xpXYQoIiIiMhip0B5gOkMR/rElQGtHmI5QlOb2Ljb+s4m6QDD5mkJvDlOOKuUwn4chRW5K\nC1wMK3Yf0jtziYiIiAx0KrQHgGgszoYPGnll4y42bG4kGttzhoUcu5VJR/o4dmwxR48upDjfmbE5\nTUVEREQkQYV2P9YVifHy2zv5fe3HyduuDi/JZcqEUkoLXbididsUjyz1HPAk9yIiIiJyYFRo91M1\nG3fxyz++T3swgt1mYdqkEZx53HBGZvAOXCIiIiLSeyq0+5muSIy1f9jEy2/vJMdh5SunHs45kw/T\nlHEiIiIifYwK7X6kPtDJqt+8w46GDkYP8XLtzDKGFLqzHZaIiIiI9ECFdj8R7Iryw+q32dnYydmT\nD+OSaV9Q37WIiIhIH6ZCux8wTZNHf/ceOxs7Ofekkcw+W7egFxEREenrNCTaD7zw2lZef6+eIw/L\n5+KzxmY7HBERERHpBRXafdymrc38+s+byc91cO3Midis+pWJiIiI9AcpW0cCgQD33Xcf//M//0ND\nQwOGYVBaWsr06dNZsGABXu++7+0un184EuORZ/+Bicm3Z06kwJOT7ZBEREREpJdSDo8uWrSII444\ngieffJK3336bt956iyeeeIKSkhIWLVp0qGIctH5bs4X65iAzThzJkSMLsh2OiIiIiKQhZaEdDAa5\n6qqrGDJkCFarFZvNxvDhw7n22mtpaWk5VDEOSjsaOvjdK1soysth5uljsh2OiIiIiKQpZaEdiUR4\n991391q+fv164vF4xoIa7OKmyeO/f49Y3OTyGUfidGhyGBEREZH+JmUFd+utt7Jw4UK6urrw+XwA\n1NXVkZ+fz/Lly/e78WXLlrFhwwYMw6CiooJjjz12r9fcc889vPXWW6xZs2af6+zcuZOFCxcSi8Xw\n+XzcfffdOBwD906If31nJ5u2tXDCuBJOGOfLdjgiIiIicgBSFtrHHXcczz33HNu3b6e+vh7DMBg6\ndChDhw7d74ZfffVVtmzZQlVVFZs3b6aiooKqqqo9XvPBBx/w2muvYbfbU66zatUqLrvsMs4//3zu\nvfdeqqurueyyyz7H2+67gl1RfvOXzTjsFi6fcWS2wxERERGRA5SydSQcDrN69WoWLlyY/Pfd736X\nn//850QikZQbrqmp4ZxzzgFg7NixtLS00N7evsdrli9fzne+8539rlNbW8vZZ58NwLRp06ipqUn/\nnfYTv6vdQmtnhPP/ZTRFec5shyMiIiIiByjliPaiRYsoLi7mpptuwufzYZom9fX1PPPMMyxZsiRl\n+0hDQwNlZWXJx0VFRfj9fjweDwDr1q1jypQpjBgxYr/rBIPBZKtIcXExfr8/5ZsqLHRjs1lTviaT\nfL4Dm/awoTnIC69upSjPyRVfOhpnzuDozT7QfA1myll6lK/0KWfpUb7Sp5ylR/lKX1/IWcpKzu/3\nc9999+2xbPTo0Zx00klpt26Yppn8ubm5mXXr1vHoo49SV1fXq3VSLfusQKAzrdgOJp/Pi9/fdkDr\nPvzbvxOOxrngtMNpaw1yYFvpXz5PvgYr5Sw9ylf6lLP0KF/pU87So3yl71DmLFVBn7LQDofD7Nq1\na6+e7K1btxKNRlPutLS0lIaGhuTj+vr65AWVr7zyCk1NTVx++eWEw2E+/vhjli1bts913G43oVAI\np9NJXV0dpaWlKffdH23Z1cbf3t3FyFIPUycOy3Y4IiIiIvI5pSy0v/3tbzNr1izGjBmzx6wj27dv\n584770y54alTp3L//fcze/ZsNm7cSGlpabJt5LzzzuO8884DYNu2bdx6661UVFTw5ptv9rjOqaee\nyvPPP88FF1zACy+8wOmnn34w3nuf8vRLHwLw9elfwGIxshyNiIiIiHxeKQvtadOm8eKLL/LWW29R\nX18PwNChQznuuOOSM4Xsy6RJkygrK2P27NkYhkFlZSXr1q3D6/UyY8aMXq8DMH/+fBYtWkRVVRXD\nhw9n5syZB/Je+6xAWxdvf9jI4UO9HH14UbbDEREREZGDwDB70/TcgxUrVvTZ27Bns4/pQHqCnq35\niN/8z4dc+cXxTDthxH5fP5Co7yx9yll6lK/0KWfpUb7Sp5ylR/lKX1/p0U45vV8qGzduPNBV5VNM\n0+Tld3Zht1n4l6MGXu+5iIiIyGCVsnXkzDPPxDD27hc2TZNAIJCxoAaTD7a3UNfUyclHD8HtTN2O\nIyIiIiL9R8pCe/LkyZx44omceeaZeyw3TZNbbrklo4ENFi+9vROA047VTCMiIiIiA0nK1pEf/OAH\n1NbWUlBQwIgRI5L/DjvssP1eDCn7FwpHee0f9RTnOZkwujDb4YiIiIjIQZRyRDs3N5cf/vCHPT73\nyCOPZCSgweT19/x0RWJ8ccpILD206IiIiIhI/9XriyHb2tpoaWlJPtaI9udXs3EXAKcdo7YRERER\nkYEm5Yg2wP/+7//ywAMP4HK5KCwspKmpiaOPPpqbbrpJxfbnEI3F+WB7C4f5cikpcGU7HBERERE5\nyFIW2rW1taxevZo77riDcePGJZe/8sor3HXXXZx77rmceOKJ2Gz7rdflMz7a2UYkGufIkQXZDkVE\nREREMiBlhfzTn/6UlStXcsstt9DY2EhZWRkTJkygqKiIrVu34vf7+e///m8uvPDCQxXvgLFpWzOA\nCm0RERGRASplj3ZXVxfDhw/nC1/4AnPnzuXrX/86wWCQhx9+mAULFjBt2jT+/Oc/H6pYB5RNWxOF\n9rjDVGiLiIiIDEQpC22r1QrAP//5Ty666CL+5V/+hQULFvDDH/6Q3//+93g8Hpqbmw9JoANJPG7y\n/rYWSgtcFHpzsh2OiIiIiGRAykLbbrcTCoVwu9288soryeXjx4/n448/BhI3r5H0bPO3E+yKqm1E\nREREZABL2aM9c+ZMHnvsMZYvX87ixYu55557GD16NNu2bePLX/4y7733HkOHDj1UsQ4YybaRkflZ\njkREREREMiVlof3lL3+ZRYsW8dRTT/Hggw/S2NjIjh07GDZsGBaLhXnz5lFZWXmoYh0wNm1LzEc+\nXiPaIiIiIgPWfuflW7FiBWvWrOGKK67g8MMPp6SkhG3btvHRRx9xyy23MGHChEMR54BhmiabtjaT\n73Hg0/zZIiIiIgNWrybAvvLKK7nyyivZvn07fr+f/Px8xowZs9/1li1bxoYNGzAMg4qKCo499tjk\nc08++STV1dVYLBYmTJhAZWUl1dXVPPPMM8nXvPvuu6xfv54rr7ySzs5O3G43AIsWLWLixInpvtc+\noT4QpLUjzJSjSjF023URERGRASutO80MHz6cYcOG9apAfPXVV9myZQtVVVVs3ryZiooKqqqqAAgG\ngzz77LOsXbsWu91OeXk569evZ9asWcyaNSu5/u9+97vk9u666y6OPPLIdMLtk/5P0/qJiIiIDAop\nZx0JBoPccccdycdnn302Rx99NMcffzybNm1KueGamhrOOeccAMaOHUtLSwvt7e0AuFwuHnvsMex2\nO8FgkPb2dnw+3x7rP/DAA1x33XUH9Kb6sve7C231Z4uIiIgMbClHtP/jP/6D5uZmYrEYVquVESNG\n8Kc//Yk///nP/OQnP+G+++7b57oNDQ2UlZUlHxcVFeH3+/F4PMllq1ev5vHHH6e8vJyRI0cml7/9\n9tsMGzZsj+J71apVBAIBxo4dS0VFBU6nc5/7Lix0Y7NZU7/zDPL5vPt87sNdbeS67Bx31FAsFrWO\nQOp8Sc+Us/QoX+lTztKjfKVPOUuP8pW+vpCzlIX266+/zm9+85vkjWt2mzZtGj/+8Y/T2lFP821f\nc801lJeXc/XVVzN58mQmT54MQHV19R63dS8vL2f8+PGMGjWKyspK1q5dy5w5c/a5r0CgM63YDiaf\nz4vf39bjc+3BCDsbOigbU0RjY/shjqxvSpUv6Zlylh7lK33KWXqUr/QpZ+lRvtJ3KHOWqqBP2Tri\n8Xiw2T6pxb/73e8mf3Y4HCl3WlpaSkNDQ/JxfX19coS6ubmZ1157DQCn08kZZ5zBm2++mXxtbW0t\nJ5xwQvLxjBkzGDVqFADTp0/fb9tKX/XPna0AHDEsL8uRiIiIiEimpSy0Ozs7iUajyce7Zw0JhUIE\ng8GUG546dSrPP/88ABs3bqS0tDTZNhKNRlm8eDEdHR0AvPPOO8lZTOrq6sjNzU0W8qZpctVVV9Ha\nmihSa2trGTduXNpvtC/4cEfiPYwZrkJbREREZKBL2Toybdo0lixZwm233UZubi4AgUCA22+/nUsu\nuSTlhidNmkRZWRmzZ8/GMAwqKytZt24dXq+XGTNmcP3111NeXo7NZmP8+PGcffbZAPj9foqKipLb\nMQyDSy65hKuuugqXy8WQIUOYP3/+533fWaERbREREZHBwzB7ap7uFo1Gueeee/jNb37D8OHDiUaj\n+P1+vvnNbzJ37txDGWdastnHtK+eINM0WbDqZZwOKyu/fWoWIuub1HeWPuUsPcpX+pSz9Chf6VPO\n0qN8pa+v9GinHNG22WwsWrSIG264gS1btmC1Whk9evR++7Nlb/6WEO3BCEcfXpjtUERERETkEEjZ\nox2Px/nxj3+Mw+FgwoQJjBs3jq1bt/KTn/zkUMU3YHy4owWAMWobERERERkUUhbaDzzwABs3biQc\nDieXDRkyhPfee4/HH38848ENJP/ckTh9cYQuhBQREREZFFIW2n/+85+57777cLlcyWUej4cVK1bw\n3HPPZTz3Aq3/AAAf/0lEQVS4geTDnS1YDINRQ7I/ebqIiIiIZF7KQtvpdPbYj+10OrFYUq4qnxKN\nxdmyq53DSnPJsWfvjpUiIiIicujsdx7tzs6977LY0tKSnANb9m+bv51oLK5p/UREREQGkZSF9gUX\nXMC8efP46KOPksvee+89rr32Wr7xjW9kOrYBQzeqERERERl8Uk7v941vfAOHw8G//du/0dbWhmma\nFBcXM3fuXGbOnHmoYuz3/rlDN6oRERERGWxSFtoAl19+OZdffjnt7e0YhpG8Q6T03j93tZHjsDKs\nWLkTERERGSz2W2i/++67/OxnP2PTpk1YLBYmTpzIN7/5TcaNG3co4uv34nGTuqZORg3xYLEY2Q5H\nRERERA6RlD3ar7/+OvPmzePUU0/lP//zP/ne977HEUccwZw5c3jjjTcOVYz9WqCti1jcxFfg2v+L\nRURERGTASDmi/dBDD/GjH/2IiRMnJpdNmjSJk08+mRUrVvDEE09kPMD+rr45CEBpoQptERERkcEk\n5Yh2MBjco8je7Zhjjulx2j/Zm7+70NaItoiIiMjgkrLQTnVTGo/Hc9CDGYjqA90j2iq0RURERAaV\nlK0j9fX1VFdX9/ic3+/PSEADjUa0RURERAanlIX2CSecsM+LHo8//vj9bnzZsmVs2LABwzCoqKjg\n2GOPTT735JNPUl1djcViYcKECVRWVvLqq6+yYMGC5IwmRx55JEuWLGHnzp0sXLiQWCyGz+fj7rvv\n7vHW8H1RfXMQm9VCgTcn26GIiIiIyCGUstC+6667DnjDr776Klu2bKGqqorNmzdTUVFBVVUVkOj9\nfvbZZ1m7di12u53y8nLWr18PwJQpU1i1atUe21q1ahWXXXYZ559/Pvfeey/V1dVcdtllBxzboeQP\nBPEVOLEYmtpPREREZDBJWWjfeuut+3zOMAyWLVu2z+dramo455xzABg7diwtLS20t7fj8XhwuVw8\n9thjQKLobm9vx+fzsWPHjh63VVtby/e+9z0Apk2bxiOPPNIvCu32YITOrijjDsvPdigiIiIicoil\nLLQvvPDCvZZ1dnby4IMPEggEUm64oaGBsrKy5OOioiL8fv8eF1GuXr2axx9/nPLyckaOHMmOHTv4\n4IMPuPbaa2lpaWHevHlMnTqVYDCYbBUpLi7eb394YaEbm82a8jWZ5PN5AWjemsjRqOH5yWWyN+Um\nfcpZepSv9Cln6VG+0qecpUf5Sl9fyFnKQnvKlCl7PP7tb3/L/fffz0UXXcQ3vvGNtHZkmuZey665\n5hrKy8u5+uqrmTx5Mocffjjz5s3j/PPPZ+vWrZSXl/PCCy/sdzufFQhkb+pBn8+L398GwKZ/NgLg\nybEml8mePp0v6R3lLD3KV/qUs/QoX+lTztKjfKXvUOYsVUG/31uwA2zatIkf/OAHlJSU8NhjjzF0\n6ND9rlNaWkpDQ0PycX19PT6fD4Dm5mbef/99TjrpJJxOJ2eccQZvvvkmkydP5ktf+hIAo0aNoqSk\nhLq6OtxuN6FQCKfTSV1dHaWlpb0JO+s0tZ+IiIjI4JVyHu329nbuuOMOFi5cyLx587jvvvt6VWQD\nTJ06leeffx6AjRs3UlpammwbiUajLF68mI6ODgDeeecdxowZwzPPPMPPfvYzIDF9YGNjI0OGDOHU\nU09NbuuFF17g9NNPP7B3e4j5dVdIERERkUEr5Yj2ueeey9ChQ7niiivYuXMnTz/99B7Pz5w5c5/r\nTpo0ibKyMmbPno1hGFRWVrJu3Tq8Xi8zZszg+uuvp7y8HJvNxvjx4zn77LPp6Ojglltu4cUXXyQS\nibB06VIcDgfz589n0aJFVFVVMXz48JT77Uv8zUEMoCTfme1QREREROQQS1loX3rppRiGwa5duw5o\n47fccssejydMmJD8+aKLLuKiiy7a43mPx8ODDz6413ZKS0t59NFHDyiGbKpvDlLgzcGexQszRURE\nRCQ7Uhba8+fPP1RxDDiRaJxAaxdHjizIdigiIiIikgUpe7TlwDW0BDEBn/qzRURERAYlFdoZsvtC\nSJ9mHBEREREZlFRoZ4im9hMREREZ3Ho1j/Zvf/tbfvrTn9La2oppmpimiWEY/OUvf8lweP1Xvab2\nExERERnUelVo33///dxxxx0MHz480/EMGA3NIUCtIyIiIiKDVa8K7dGjR3PSSSdlOpYBpb45iDvH\nhsdlz3YoIiIiIpIFvSq0TzjhBO69916mTJmC1frJnNCnnHJKxgLr7wJtIYrzNJotIiIiMlj1qtD+\n29/+BsD69euTywzDUKG9D6ZpEuqK4crRjWpEREREBqteFdpr1qzJdBwDSjgSxwScjl6lV0REREQG\noF5N77d582bKy8uZNGkSkydPZs6cOXz88ceZjq3fCoWjADgdGtEWERERGax6VWj/4Ac/4Jvf/CYv\nv/wy//u//8vs2bOprKzMdGz9VigcA1Roi4iIiAxmvSq0TdPkrLPOwu12k5uby4wZM4jFYpmOrd/6\npNBW64iIiIjIYNWrQjsSibBx48bk47fffluFdgpqHRERERGRXg25Llq0iJtvvpmmpiZM06S0tJTl\ny5dnOrZ+S60jIiIiItKrQvu4447j97//PW1tbRiGgcfj6dXGly1bxoYNGzAMg4qKCo499tjkc08+\n+STV1dVYLBYmTJhAZWUlhmGwcuVK3njjDaLRKHPnzuXcc89l8eLFbNy4kYKCAgDmzJnDWWedlf67\nPURUaIuIiIhIykL7oYceYu7cuXz3u9/FMIy9nl+5cuU+13311VfZsmULVVVVbN68mYqKCqqqqgAI\nBoM8++yzrF27FrvdTnl5OevXryccDvP+++9TVVVFIBDgwgsv5NxzzwXgpptuYtq0aZ/nvR4yn7SO\nqEdbREREZLBKWQkeffTRAJx66ql7PddT4f1pNTU1nHPOOQCMHTuWlpYW2tvb8Xg8uFwuHnvsMSBR\ndLe3t+Pz+Rg+fHhy1DsvL49gMNgve8E1oi0iIiIiKQvt008/HUjMo33LLbfs8dy///u/M3PmzH2u\n29DQQFlZWfJxUVERfr9/j7aT1atX8/jjj1NeXs7IkSMBcLvdAFRXV3PGGWckb/n+xBNP8Oijj1Jc\nXMySJUsoKira574LC93YbNkrcq3d+x5S6sXn82Ytjv5COUqfcpYe5St9yll6lK/0KWfpUb7S1xdy\nlrLQ/sMf/sALL7xATU0N9fX1yeXRaJTXXnstrR2ZprnXsmuuuYby8nKuvvpqJk+ezOTJkwH44x//\nSHV1NY888ggAF1xwAQUFBRx11FGsXr2aH/3oR9x+++373Fcg0JlWbAeTz+elqTkIQKgzjN/flrVY\n+gOfz6scpUk5S4/ylT7lLD3KV/qUs/QoX+k7lDlLVdDvd0S7qKiId999l1NOOSW53DAM5s2bl3Kn\npaWlNDQ0JB/X19fj8/kAaG5u5v333+ekk07C6XRyxhln8OabbzJ58mReeuklHnzwQR5++GG83kTg\nn9739OnTWbp0acp9Z5um9xMRERGRlPNoO51OJk+ezNNPP82FF16Y/Ddz5kx+/etfp9zw1KlTef75\n5wHYuHEjpaWlybaRaDTK4sWL6ejoAOCdd95hzJgxtLW1sXLlSh566KHkDCMA8+fPZ+vWrQDU1tYy\nbty4A3/Hh4B6tEVERESkV9NivP7669x77700NzcDEA6HKSgoYNGiRftcZ9KkSZSVlTF79mwMw6Cy\nspJ169bh9XqZMWMG119/PeXl5dhsNsaPH8/ZZ5/Nk08+SSAQ4MYbb0xuZ8WKFVx++eXceOONuFwu\n3G43d9111+d825mlO0OKiIiIiGH21Dz9GbNmzeLf//3fWbZsGXfeeSfPPfccJ554IlOnTj0UMaYt\nm31MPp+X7/7wf3jv42YeXjgNiyX17CyDnfrO0qecpUf5Sp9ylh7lK33KWXqUr/T1lR7tXt2C3ePx\ncPzxx2O32xk3bhwLFizg0UcfPWgBDjTBcAyHzaIiW0RERGQQ61VvQzQa5fXXXycvL4+nnnqKsWPH\nsm3btkzH1m91hWPqzxYREREZ5HpVaH/ve9+joaGBhQsX8oMf/ICGhgauvfbaTMfWb4XCUfVni4iI\niAxyvaoGjzjiCI444giA5NzWsm+hcIw8tyPbYYiIiIhIFqUstKdPn57yVusvvvjiQQ+ovzNNU60j\nIiIiIpK60P75z38OQFVVFT6fj5NPPplYLMZf//pXOjuzd/fFviwUjmECOWodERERERnUUlaDo0aN\nAuDvf//7HrOMlJWVMXfu3MxG1k8Fu3RXSBERERHp5fR+jY2NvPzyy3R2dhIKhaipqWHHjh2Zjq1f\nCqnQFhERERF6eTHk0qVLWblyJZs2bcI0TcaNG8eSJUsyHVu/1JkstNU6IiIiIjKY9aoanDRpEr/6\n1a8yHcuAoNYREREREYH9FNp33HEHt912G5dddlmPs4+sXbs2Y4H1Vyq0RURERAT2U2hffPHFANx4\n442HJJiBIBhSoS0iIiIi+ym0A4EANTU1hyqWASGoHm0RERERYT+F9o9//ON9PmcYBqeccspBD6i/\nC4U1oi0iIiIi+ym016xZs8/nnn/++YMezECg1hERERERgV7OOrJjxw6eeOIJAoEAAOFwmNraWr74\nxS+mXG/ZsmVs2LABwzCoqKjg2GOPTT735JNPUl1djcViYcKECVRWVmIYRo/r7Ny5k4ULFxKLxfD5\nfNx99904HI7P8bYzJzm9X45aR0REREQGs17dsGbhwoUUFBTw1ltvMXHiRAKBACtXrky5zquvvsqW\nLVuoqqrizjvv5M4770w+FwwGefbZZ1m7di2/+tWv+PDDD1m/fv0+11m1ahWXXXYZv/jFLxg9ejTV\n1dWf4y1n1u4e7Ry7RrRFREREBrNeFdpWq5VrrrmGkpISLr/8cn7yk5/sd2q/mpoazjnnHADGjh1L\nS0sL7e3tALhcLh577DHsdjvBYJD29nZ8Pt8+16mtreXss88GYNq0aX36Ak1N7yciIiIi0MvWka6u\nLnbt2oVhGGzdupXhw4ezffv2lOs0NDRQVlaWfFxUVITf78fj8SSXrV69mscff5zy8nJGjhy5z3WC\nwWCyVaS4uBi/359y34WFbmy27BS6oa4YAIcNLyDXZc9KDP2Nz+fNdgj9jnKWHuUrfcpZepSv9Cln\n6VG+0tcXctarQvtb3/oWNTU1zJkzhwsuuACr1cpXvvKVtHZkmuZey6655hrKy8u5+uqrmTx5cq/W\n6WnZZwUCnWnFdjDtHtFubw3S2R7KWhz9hc/nxe9vy3YY/Ypylh7lK33KWXqUr/QpZ+lRvtJ3KHOW\nqqBPWWjX1dUxZMiQZDsHJHqvOzo6yM/PT7nT0tJSGhoako/r6+vx+XwANDc38/7773PSSSfhdDo5\n44wzePPNN/e5jtvtJhQK4XQ6qauro7S0NPU7zqJgVwSH3YLFsvedNEVERERk8EjZo/3Vr36Va665\nhhdeeIFoNDFSa7PZ9ltkA0ydOjU5BeDGjRspLS1Nto1Eo1EWL15MR0cHAO+88w5jxozZ5zqnnnpq\ncvkLL7zA6aeffoBvN/OCXVGcuhBSREREZNBLOaL90ksv8Yc//IEnn3yS73//+3z1q1/l4osvZuzY\nsfvd8KRJkygrK2P27NkYhkFlZSXr1q3D6/UyY8YMrr/+esrLy7HZbIwfP56zzz4bwzD2Wgdg/vz5\nLFq0iKqqKoYPH87MmTMPzrvPgGBXVHeFFBEREREMszdNzyTaOP77v/+b//qv/8LtdnPxxRdz8cUX\nZzq+A5LNPqbr7/sffPkuln5zStZi6E/Ud5Y+5Sw9ylf6lLP0KF/pU87So3ylr6/0aPdqej9I9FzP\nmTOH++67jxEjRvD973//oAQ3kMRNk1A4pqn9RERERKR3s460tLTw29/+lqeeeopwOMzFF1/Mbbfd\nlunY+p1wJIZp6q6QIiIiIrKfQvtPf/oTTz31FG+88QYzZszg9ttv3+M26rKnUDgxh7buCikiIiIi\nKQvtRx55hIsvvpi7774bp9N5qGLqt3YX2modEREREZGUhfYTTzxxqOIYEELh3bdfV+uIiIiIyGDX\n64shZf92335dI9oiIiIiokL7IApFugvtHBXaIiIiIoOdCu2DKNk6ooshRURERAY9FdoH0ScXQ6pH\nW0RERGSwU6F9EKlHW0RERER2U6F9EH0y64gKbREREZHBToX2QdSVvBhSrSMiIiIig50K7YNIN6wR\nERERkd1UaB9EugW7iIiIiOymQvsgCnXpzpAiIiIikpDRinDZsmVs2LABwzCoqKjg2GOPTT73yiuv\ncO+992KxWBgzZgx33nknv/nNb3jmmWeSr3n33XdZv349V155JZ2dnbjdbgAWLVrExIkTMxn6AVHr\niIiIiIjslrFC+9VXX2XLli1UVVWxefNmKioqqKqqSj5/++238/jjjzN06FBuuOEGXnrpJWbNmsWs\nWbOS6//ud79Lvv6uu+7iyCOPzFS4B0UoHCPHYcViMbIdioiIiIhkWcZaR2pqajjnnHMAGDt2LC0t\nLbS3tyefX7duHUOHDgWgqKiIQCCwx/oPPPAA1113XabCy4hQJIZLM46IiIiICBkc0W5oaKCsrCz5\nuKioCL/fj8fjAUj+t76+nr/+9a8sWLAg+dq3336bYcOG4fP5kstWrVpFIBBg7NixVFRU4HQ697nv\nwkI3Ntuhb9+IRGO4HDZ8Pu8h33d/pnylTzlLj/KVPuUsPcpX+pSz9Chf6esLOTtkw6+mae61rLGx\nkWuvvZbKykoKCwuTy6urq7nwwguTj8vLyxk/fjyjRo2isrKStWvXMmfOnH3uKxDoPLjB91JHKEqB\nx4nf35aV/fdHPp9X+UqTcpYe5St9yll6lK/0KWfpUb7Sdyhzlqqgz1jrSGlpKQ0NDcnH9fX1e4xQ\nt7e3c/XVV3PjjTdy2mmn7bFubW0tJ5xwQvLxjBkzGDVqFADTp09n06ZNmQr7c8mxWfAVurIdhoiI\niIj0ARkrtKdOncrzzz8PwMaNGyktLU22iwAsX76cf/u3f+OMM87YY726ujpyc3NxOBxAYiT8qquu\norW1FUgU4ePGjctU2J/L7VedxHcunZTtMERERESkD8hY68ikSZMoKytj9uzZGIZBZWUl69atw+v1\nctppp/H000+zZcsWqqurAfjKV77C17/+dfx+P0VFRcntGIbBJZdcwlVXXYXL5WLIkCHMnz8/U2F/\nLkV5TnJddjrbQ9kORURERESyzDB7ap7u57LZx6Q+qvQoX+lTztKjfKVPOUuP8pU+5Sw9ylf6BnyP\ntoiIiIjIYKZCW0REREQkA1Roi4iIiIhkgAptEREREZEMGJAXQ4qIiIiIZJtGtEVEREREMkCFtoiI\niIhIBqjQFhERERHJABXaIiIiIiIZoEJbRERERCQDVGiLiIiIiGSALdsBDBTLli1jw4YNGIZBRUUF\nxx57bLZD6pNWrlzJG2+8QTQaZe7cufzpT39i48aNFBQUADBnzhzOOuus7AbZh9TW1rJgwQLGjRsH\nwJFHHsm3vvUtFi5cSCwWw+fzcffdd+NwOLIcad/w61//mmeeeSb5+N1332XixIl0dnbidrsBWLRo\nERMnTsxWiH3Gpk2buO6667jqqqu44oor2LlzZ4/H1TPPPMNjjz2GxWLhkksuYdasWdkOPWt6ytmt\nt95KNBrFZrNx99134/P5KCsrY9KkScn1fv7zn2O1WrMYeXZ8Nl+LFy/u8fNex9gnPpuzG264gUAg\nAEBzczPHH388c+fO5atf/Wryc6ywsJBVq1ZlM+ys+WxNccwxx/S9zzFTPrfa2lrzmmuuMU3TND/4\n4APzkksuyXJEfVNNTY35rW99yzRN02xqajLPPPNMc9GiReaf/vSnLEfWd73yyivm/Pnz91i2ePFi\n87nnnjNN0zTvuecec+3atdkIrc+rra01ly5dal5xxRXm//3f/2U7nD6lo6PDvOKKK8zbbrvNXLNm\njWmaPR9XHR0d5rnnnmu2traawWDQ/PKXv2wGAoFshp41PeVs4cKF5rPPPmuapmk+8cQT5ooVK0zT\nNM0pU6ZkLc6+oqd89fR5r2PsEz3l7NMWL15sbtiwwdy6dat54YUXZiHCvqWnmqIvfo6pdeQgqKmp\n4ZxzzgFg7NixtLS00N7enuWo+p6TTjqJH/7whwDk5eURDAaJxWJZjqr/qa2t5eyzzwZg2rRp1NTU\nZDmivumBBx7guuuuy3YYfZLD4eCnP/0ppaWlyWU9HVcbNmzgmGOOwev14nQ6mTRpEm+++Wa2ws6q\nnnJWWVnJF7/4RSAxqtjc3Jyt8PqcnvLVEx1jn0iVsw8//JC2tjadLf+UnmqKvvg5pkL7IGhoaKCw\nsDD5uKioCL/fn8WI+iar1Zo8fV9dXc0ZZ5yB1WrliSeeoLy8nO985zs0NTVlOcq+54MPPuDaa6/l\n0ksv5a9//SvBYDDZKlJcXKxjrQdvv/02w4YNw+fzAbBq1Souv/xybr/9dkKhUJajyz6bzYbT6dxj\nWU/HVUNDA0VFRcnXDObPtp5y5na7sVqtxGIxfvGLX/DVr34VgHA4zM0338zs2bN59NFHsxFu1vWU\nL2Cvz3sdY5/YV84AHn/8ca644ork44aGBm644QZmz569R7vcYNJTTdEXP8fUo50Bpu5qn9If//hH\nqqureeSRR3j33XcpKCjgqKOOYvXq1fzoRz/i9ttvz3aIfcbhhx/OvHnzOP/889m6dSvl5eV7nAXQ\nsdaz6upqLrzwQgDKy8sZP348o0aNorKykrVr1zJnzpwsR9i37eu40vG2t1gsxsKFCzn55JM55ZRT\nAFi4cCH/+q//imEYXHHFFZx44okcc8wxWY40+y644IK9Pu9POOGEPV6jY2xv4XCYN954g6VLlwJQ\nUFDAggUL+Nd//Vfa2tqYNWsWJ5988n7PHgxUn64pzj333OTyvvI5phHtg6C0tJSGhobk4/r6+uRI\nmuzppZde4sEHH+SnP/0pXq+XU045haOOOgqA6dOns2nTpixH2LcMGTKEL33pSxiGwahRoygpKaGl\npSU5KltXVzdoP1xTqa2tTf4BnzFjBqNGjQJ0jKXidrv3Oq56+mzT8banW2+9ldGjRzNv3rzksksv\nvZTc3Fzcbjcnn3yyjrluPX3e6xjbv9dee22PlhGPx8PXvvY17HY7RUVFTJw4kQ8//DCLEWbPZ2uK\nvvg5pkL7IJg6dSrPP/88ABs3bqS0tBSPx5PlqPqetrY2Vq5cyUMPPZS86nz+/Pls3boVSBRHu2fX\nkIRnnnmGn/3sZwD4/X4aGxu56KKLksfbCy+8wOmnn57NEPucuro6cnNzcTgcmKbJVVddRWtrK6Bj\nLJVTTz11r+PquOOO45133qG1tZWOjg7efPNNTjzxxCxH2nc888wz2O12brjhhuSyDz/8kJtvvhnT\nNIlGo7z55ps65rr19HmvY2z/3nnnHSZMmJB8/Morr3DXXXcB0NnZyXvvvceYMWOyFV7W9FRT9MXP\nMbWOHASTJk2irKyM2bNnYxgGlZWV2Q6pT3ruuecIBALceOONyWUXXXQRN954Iy6XC7fbnfzwkITp\n06dzyy238OKLLxKJRFi6dClHHXUUixYtoqqqiuHDhzNz5sxsh9mn+P3+ZD+eYRhccsklXHXVVbhc\nLoYMGcL8+fOzHGH2vfvuu6xYsYLt27djs9l4/vnn+Y//+A8WL168x3Flt9u5+eabmTNnDoZhcP31\n1+P1erMdflb0lLPGxkZycnK48sorgcTF8EuXLmXo0KFcfPHFWCwWpk+fPigvYOspX1dcccVen/dO\np1PHWLeecnb//ffj9/uTZ+UATjzxRJ5++mm+/vWvE4vFuOaaaxgyZEgWI8+OnmqK5cuXc9ttt/Wp\nzzHDVEOUiIiIiMhBp9YREREREZEMUKEtIiIiIpIBKrRFRERERDJAhbaIiIiISAao0BYRERERyQBN\n7yciMgBs27aN8847b6877Z155pl861vf+tzbr62t5T//8z/55S9/+bm3JSIyWKjQFhEZIIqKiliz\nZk22wxARkW4qtEVEBrijjz6a6667jtraWjo6Oli+fDlHHnkkGzZsYPny5dhsNgzD4Pbbb+cLX/gC\nH330EUuWLCEej5OTk5O8kVQ8HqeyspJ//OMfOBwOHnroIQBuvvlmWltbiUajTJs2jW9/+9vZfLsi\nIn2GerRFRAa4WCzGuHHjWLNmDZdeeimrVq0CYOHChdx6662sWbOGb3zjG3zve98DoLKykjlz5rB2\n7Vq+9rWv8bvf/Q6AzZs3M3/+fJ588klsNhsvv/wyf/vb34hGo/ziF7/gV7/6FW63m3g8nrX3KiLS\nl2hEW0RkgGhqakreDny37373uwCcdtppAEyaNImf/exntLa20tjYmLw9+JQpU7jpppsAePvtt5ky\nZQoAX/7yl4FEj/YRRxxBSUkJAEOHDqW1tZXp06ezatUqFixYwJlnnsmsWbOwWDSGIyICKrRFRAaM\nVD3apmkmfzYMA8Mw9vk80OOotNVq3WtZcXEx//Vf/8X69et58cUX+drXvsZTTz2F0+k8kLcgIjKg\naNhBRGQQeOWVVwB44403GD9+PF6vF5/Px4YNGwCoqanh+OOPBxKj3i+99BIAzz33HPfee+8+t/vy\nyy/zl7/8hcmTJ7Nw4ULcbjeNjY0ZfjciIv2DRrRFRAaInlpHDjvsMAD+/ve/88tf/pKWlhZWrFgB\nwIoVK1i+fDlWqxWLxcLSpUsBWLJkCUuWLOEXv/gFNpuNZcuW8fHHH/e4zzFjxrB48WIefvhhrFYr\np512GiNGjMjcmxQR6UcM87PnC0VEZEAZP348GzduxGbT2IqIyKGk1hERERERkQzQiLaIiIiISAZo\nRFtEREREJANUaIuIiIiIZIAKbRERERGRDFChLSIiIiKSASq0RUREREQyQIW2iIiIiEgG/D8NMbZ7\nOB7bfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6de861690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size)\n",
    "saver, logits_var, _, _, _ = dae.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
