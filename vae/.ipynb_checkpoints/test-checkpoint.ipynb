{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders for collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
    "\n",
    "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create train/validation/test splits following strong generalization: \n",
    "\n",
    "- We split all users into training/validation/test sets. \n",
    "\n",
    "- We train models using the entire click history of the training users. \n",
    "\n",
    "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset at http://files.grouplens.org/datasets/movielens/ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
    "DATA_DIR = '/Users/andyliu/ml-20m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112485573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n",
    "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" %\n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize_old(tp):\n",
    "    uid = map(lambda x: profile2id[x], tp['userId'])\n",
    "    sid = map(lambda x: show2id[x], tp['movieId'])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "def numerize(tp):\n",
    "    #https://github.com/dawenl/vae_cf/issues/10\n",
    "    uid = [profile2id[x] for x in tp['userId']]\n",
    "    sid = [show2id[x] for x in tp['movieId']]\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDAE(object):\n",
    "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "\n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.construct_placeholders()\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        self.input_ph = tf.compat.v1.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
    "        self.keep_prob_ph = tf.compat.v1.placeholder_with_default(1.0, shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.construct_weights()\n",
    "\n",
    "        saver, logits = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        # per-user average negative log-likelihood\n",
    "        neg_ll = -tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "            input_tensor=log_softmax_var * self.input_ph, axis=1))\n",
    "        # apply regularization to weights\n",
    "        #OLD REGULARIZATION (NOT USEFUL)\n",
    "        '''reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)'''\n",
    "        \n",
    "        reg = tf.keras.regularizers.l2(self.lam)\n",
    "        penalties = [reg(w) for w in self.weights]\n",
    "        penalties = [p if p is not None else constant_op.constant(0.0) for p in penalties]\n",
    "        for p in penalties:\n",
    "            if p.get_shape().ndims != 0:\n",
    "                raise ValueError('regularizer must return a scalar Tensor instead of a '\n",
    "                         'Tensor with rank %d.' % p.get_shape().ndims)\n",
    "        summed_penalty = tf.math.add_n(penalties)\n",
    "        reg_var = summed_penalty\n",
    "        \n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        loss = neg_ll + 2 * reg_var\n",
    "\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.compat.v1.summary.scalar('loss', loss)\n",
    "        merged = tf.compat.v1.summary.merge_all()\n",
    "        return saver, logits, loss, train_op, merged\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # construct forward graph\n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, 1 - (self.keep_prob_ph))\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return tf.compat.v1.train.Saver(), h\n",
    "\n",
    "    def construct_weights(self):\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # define weights\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
    "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_{}\".format(i+1)\n",
    "\n",
    "            self.weights.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(MultiDAE):\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        super(MultiVAE, self).construct_placeholders()\n",
    "\n",
    "        # placeholders with default values when scoring\n",
    "        self.is_training_ph = tf.compat.v1.placeholder_with_default(0., shape=None)\n",
    "        self.anneal_ph = tf.compat.v1.placeholder_with_default(1., shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._construct_weights()\n",
    "\n",
    "        saver, logits, KL = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        neg_ll = -tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "            input_tensor=log_softmax_var * self.input_ph,\n",
    "            axis=-1))\n",
    "        # apply regularization to weights\n",
    "        '''reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)'''\n",
    "        \n",
    "        reg = tf.keras.regularizers.l2(self.lam)\n",
    "        penalties = [reg(w) for w in self.weights_q + self.weights_p]\n",
    "        penalties = [p if p is not None else constant_op.constant(0.0) for p in penalties]\n",
    "        for p in penalties:\n",
    "            if p.get_shape().ndims != 0:\n",
    "                raise ValueError('regularizer must return a scalar Tensor instead of a '\n",
    "                         'Tensor with rank %d.' % p.get_shape().ndims)\n",
    "        summed_penalty = tf.math.add_n(penalties)\n",
    "        reg_var = summed_penalty\n",
    "        \n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
    "\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.compat.v1.summary.scalar('KL', KL)\n",
    "        tf.compat.v1.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
    "        merged = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "        return saver, logits, neg_ELBO, train_op, merged\n",
    "\n",
    "    def q_graph(self):\n",
    "        mu_q, std_q, KL = None, None, None\n",
    "\n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, 1 - (self.keep_prob_ph))\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights_q) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                mu_q = h[:, :self.q_dims[-1]]\n",
    "                logvar_q = h[:, self.q_dims[-1]:]\n",
    "\n",
    "                std_q = tf.exp(0.5 * logvar_q)\n",
    "                KL = tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
    "                        input_tensor=0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
    "        return mu_q, std_q, KL\n",
    "\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "\n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "\n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # q-network\n",
    "        mu_q, std_q, KL = self.q_graph()\n",
    "        epsilon = tf.random.normal(tf.shape(input=std_q))\n",
    "\n",
    "        sampled_z = mu_q + self.is_training_ph *\\\n",
    "            epsilon * std_q\n",
    "\n",
    "        # p-network\n",
    "        logits = self.p_graph(sampled_z)\n",
    "\n",
    "        return tf.compat.v1.train.Saver(), logits, KL\n",
    "\n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            if i == len(self.q_dims[:-1]) - 1:\n",
    "                # we need two sets of parameters for mean and variance,\n",
    "                # respectively\n",
    "                d_out *= 2\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "\n",
    "            self.weights_q.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases_q.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases_q[-1])\n",
    "\n",
    "        self.weights_p, self.biases_p = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.compat.v1.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
    "\n",
    "            self.biases_p.append(tf.compat.v1.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "\n",
    "            # add summary stats\n",
    "            tf.compat.v1.summary.histogram(weight_key, self.weights_p[-1])\n",
    "            tf.compat.v1.summary.histogram(bias_key, self.biases_p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = range(N)\n",
    "\n",
    "# training batch size\n",
    "batch_size = 500\n",
    "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
    "\n",
    "N_vad = vad_data_tr.shape[0]\n",
    "idxlist_vad = range(N_vad)\n",
    "\n",
    "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
    "batch_size_vad = 2000\n",
    "\n",
    "# the total number of gradient updates for annealing\n",
    "total_anneal_steps = 200000\n",
    "# largest annealing parameter\n",
    "anneal_cap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-VAE^{PR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dims = [200, 600, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.compat.v1.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.compat.v1.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory\n",
    "\n",
    "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
    "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /Users/andyliu/Downloads/vae_cf-master/200.0\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(log_dir, graph=tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /Users/andyliu/Downloads/vae_cf-master/200.0\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    update_count = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(list(idxlist))\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "\n",
    "            feed_dict = {vae.input_ph: X,\n",
    "                         vae.keep_prob_ph: 0.5,\n",
    "                         vae.anneal_ph: anneal,\n",
    "                         vae.is_training_ph: 1}\n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train,\n",
    "                                           global_step=epoch * batches_per_epoch + bnum)\n",
    "\n",
    "            update_count += 1\n",
    "\n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "\n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579008\n"
     ]
    }
   ],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "\n",
    "batch_size_test = 2000\n",
    "print(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0)\n",
    "saver, logits_var, _, _, _ = vae.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /volmount/chkpt/ml-20m/VAE_anneal200K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.42592 (0.00211)\n",
      "Test Recall@20=0.39535 (0.00270)\n",
      "Test Recall@50=0.53540 (0.00284)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> n_items] MLP, thus the overall architecture for the Multi-DAE is [n_items -> 200 -> n_items]. We find this architecture achieves better validation NDCG@100 than the [n_items -> 600 -> 200 -> 600 -> n_items] architecture as used in Multi-VAE^{PR}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dims = [200, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.compat.v1.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.compat.v1.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in dae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /Users/andyliu/Downloads/vae_cf-master/I-200-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.compat.v1.summary.FileWriter(log_dir, graph=tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /Users/andyliu/ml-20m/DAE/I-200-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/Users/andyliu/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/50\n",
      "epoch 2/50\n",
      "epoch 3/50\n",
      "epoch 4/50\n",
      "epoch 5/50\n",
      "epoch 6/50\n",
      "epoch 7/50\n",
      "epoch 8/50\n",
      "epoch 9/50\n",
      "epoch 10/50\n",
      "epoch 11/50\n",
      "epoch 12/50\n",
      "epoch 13/50\n",
      "epoch 14/50\n",
      "epoch 15/50\n",
      "epoch 16/50\n",
      "epoch 17/50\n",
      "epoch 18/50\n",
      "epoch 19/50\n",
      "epoch 20/50\n",
      "epoch 21/50\n",
      "epoch 22/50\n",
      "epoch 23/50\n",
      "epoch 24/50\n",
      "epoch 25/50\n",
      "epoch 26/50\n",
      "epoch 27/50\n",
      "epoch 28/50\n",
      "epoch 29/50\n",
      "epoch 30/50\n",
      "epoch 31/50\n",
      "epoch 32/50\n",
      "epoch 33/50\n",
      "epoch 34/50\n",
      "epoch 35/50\n",
      "epoch 36/50\n",
      "epoch 37/50\n",
      "epoch 38/50\n",
      "epoch 39/50\n",
      "epoch 40/50\n",
      "epoch 41/50\n",
      "epoch 42/50\n",
      "epoch 43/50\n",
      "epoch 44/50\n",
      "epoch 45/50\n",
      "epoch 46/50\n",
      "epoch 47/50\n",
      "epoch 48/50\n",
      "epoch 49/50\n",
      "epoch 50/50\n"
     ]
    }
   ],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"epoch \" + str(epoch+1) + \"/\" + str(n_epochs))\n",
    "        np.random.shuffle(list(idxlist))\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "\n",
    "            feed_dict = {dae.input_ph: X,\n",
    "                         dae.keep_prob_ph: 0.5}\n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum)\n",
    "\n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "            \n",
    "\n",
    "            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "\n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size)\n",
    "saver, logits_var, _, _, _ = dae.build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /Users/andyliu/Downloads/vae_cf-master/I-200-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/Users/andyliu/Downloads/vae_cf-master/{}'.format(arch_str)\n",
    "#chkpt_dir = '/Users/andyliu/ml-20m/DAE/{}'.format(arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/andyliu/Downloads/vae_cf-master/I-200-I/model\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n            ...\\n            1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\\n           dtype='int64', length=2000)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-62cba8db352f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(idxl)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(test_data_tr.head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\\n            ...\\n            1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\\n           dtype='int64', length=2000)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        #test = idxlist_test[st_idx:end_idx]\n",
    "        X = test_data_tr.iloc[list(test)]\n",
    "        idxl = idxlist_test[st_idx:end_idx]\n",
    "        #print(idxl)\n",
    "        #print(test_data_tr.head)\n",
    "        X = test_data_tr[idxl]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "        pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=nan (nan)\n",
      "Test Recall@20=nan (nan)\n",
      "Test Recall@50=nan (nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "//anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "//anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "//anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "//anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
